{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPLcs2JVlKTN4A+wofaras6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onism/MyLearning/blob/master/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1FUeg3mhBMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import spacy\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchtext\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL4E09VikpeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 1\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CE1uHhclkcm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "234e1150-b74a-494f-975b-a0a916a5ae21"
      },
      "source": [
        "!python -m spacy download de\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting de_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9MB 3.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (47.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.4.5.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp36-none-any.whl size=14907056 sha256=1cf02b248a2ffb5b866103febc3c41defdfd167c3bf701e6acc45fd4be6fb469\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dxosjxy8/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK-1ttUlkudp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use spacy for tokening the data\n",
        "spacy_en = spacy.load('en')\n",
        "spacy_de = spacy.load('de')\n",
        "\n",
        "def tokenize_de(text):\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1] ## list[::-1] used to reverse the list\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE-lTXwLl0iT",
        "colab_type": "text"
      },
      "source": [
        "German is the src (input) English is the trg (output)\n",
        "\n",
        "Append <sos> (start of sentence), <eos>(end of sentence) tokens to all sentences. This can be done simply by specifing the init_token, eos_token arguments in the field"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeIJBLRLlrUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)\n",
        "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzBb5IeKmma6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "6bd90381-9c05-4f23-a571-436e64e3cf8d"
      },
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SRC, TRG))\n",
        "print('Loaded data...')\n",
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:01<00:00, 803kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 244kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 234kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded data...\n",
            "Number of training examples: 29000\n",
            "Number of validation examples: 1014\n",
            "Number of testing examples: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJqkhB0Imzgg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "49b407ac-f4b0-4b64-beea-ecac1cbb8d6e"
      },
      "source": [
        "print(f\"src: {vars(train_data.examples[0])['src']}\")\n",
        "print(f\"trg: {vars(train_data.examples[0])['trg']}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src: ['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei']\n",
            "trg: ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7SguOODm5u0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c18ddc43-fe69-4573-f416-b5d9edacac7a"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq=2)\n",
        "TRG.build_vocab(train_data, min_freq=2)\n",
        "print('Vocab builded...')\n",
        "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")\n",
        "\n",
        "BATCH_SIZE = 32"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab builded...\n",
            "Unique tokens in source (de) vocabulary: 7855\n",
            "Unique tokens in target (en) vocabulary: 5893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvAbDrHenNsP",
        "colab_type": "text"
      },
      "source": [
        "We use BucketIterator instead of the standard Iterator as it creates batches in such a way that it minimizes the amount of padding in both the sourcee and target sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blOxfO4hnI44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxslkY9HoDyJ",
        "colab_type": "text"
      },
      "source": [
        "![Multi-Head Attention Layer](https://ravirajag.dev/assets/images/transformer/multihead.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dQHs0jYn_hS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://ravirajag.dev/machine%20learning/data%20science/deep%20learning/attention/transformers/sequence-to-sequence/2019/03/22/transformerimp.html\n",
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim \n",
        "        self.n_heads = n_heads \n",
        "        self.w_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.w_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.w_v = nn.Linear(hid_dim, hid_dim)\n",
        "\n",
        "        self.do = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc = nn.Linear(hid_dim, hid_dim)\n",
        "        # scale factor to be applied in calculation of self-attention. This is the sqrt of dimension of key vector.\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim // n_heads])).to(device)\n",
        "    \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "\n",
        "        batch_size,_, hidden_dim = query.shape \n",
        "        assert self.hid_dim == hidden_dim, \"Hidden dim must match\"\n",
        "\n",
        "        Q = self.w_q(query)\n",
        "        K = self.w_k(key)\n",
        "        V = self.w_v(value)\n",
        "        # Q, K, V: [batch_size, sent_len, hidden_dim]\n",
        "\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.hid_dim//self.n_heads).permute(0,2,1,3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.hid_dim//self.n_heads).permute(0,2,1,3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.hid_dim//self.n_heads).permute(0,2,1,3)\n",
        "        # Q, K, V: [batch_size, n_heads, sent_len, hid_dim//n_heads]\n",
        "\n",
        "        # z = softmax( (Q,K)/sqrt(q_dim)   ).V\n",
        "\n",
        "        energy = torch.matmul(  Q,  K.permute(0,1,3,2) ) / self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = self.do( F.softmax(energy, dim=-1) )\n",
        "        x = x.permute(0,2,1,3).contiguous()\n",
        "        # x: [batch_size, sent_len, n_heads, hid_dim//h_heads]\n",
        "\n",
        "        # combine all heads\n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHaGfD_m7mWK",
        "colab_type": "text"
      },
      "source": [
        "**Positionwise Feedforward Layer**\n",
        "The second layer present in Encoder is a position-wise feed forward layer.\n",
        "\n",
        "A Feed Forward Network is applied to each position separately and identically, containing 1 hidden_layer with ReLU activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXx126tt72YG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionwiseFeedforward(nn.Module):\n",
        "\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim \n",
        "        self.pf_dim  = pf_dim \n",
        "\n",
        "        self.fc_1 = nn.Conv1d(hid_dim, pf_dim, 1)\n",
        "        self.fc_2 = nn.Conv1d(pf_dim, hid_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x: batch_size, sent_len, hidden_dim\n",
        "\n",
        "        x = x.permute(0,2,1)\n",
        "\n",
        "        x = self.dropout(F.relu(self.fc_1(x)))\n",
        "        # x : [batch_size, pf_dim, sent_len]\n",
        "        x = self.fc_2(x)\n",
        "        # x: batch_size, hid_dim, sent_len\n",
        "        x = x.permute(0,2,1)\n",
        "        return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b24FyiGyBiDD",
        "colab_type": "text"
      },
      "source": [
        "**Positional Encoding**\n",
        "\n",
        "Since the transformer contains no recurrence and no covolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEvqmtJ_BcwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout, device, max_len=1000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(max_len, d_model).to(device)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x + Variable(self.pe[:,:x.size(1)], requires_grad=False)\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHAji7W0EuLP",
        "colab_type": "text"
      },
      "source": [
        "Each encoder block contains layers:\n",
        "1. self-attention layer followed by layer normalization\n",
        "2. positionwise feed-forward layer followd by layer normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I-jv11NCo5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, hid_dim, n_heads, pf_dim, self_attention, positionwise_feedward, dropout, device):\n",
        "        super().__init__()\n",
        "        self.sa = self_attention(hid_dim, n_heads, dropout, device)\n",
        "        self.pf = positionwise_feedward(hid_dim, pf_dim, dropout)\n",
        "        self.ln = nn.LayerNorm(hid_dim)\n",
        "        self.do = nn.Dropout(dropout)\n",
        "\n",
        "    \n",
        "    def forward(self, src, src_mask):\n",
        "        src = self.ln( src + self.do(self.sa(src, src, src, src_mask)))\n",
        "        src = self.ln(src + self.do(self.src))\n",
        "        return src\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuG99UO2HEdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoder contains multiple EncoderLayers. N=6 given in paper.\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pd_dim, encoder_layer, self_attention, positionwise_feedforward,\n",
        "                 positional_encoding, dropout, device):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim \n",
        "        self.hid_dim = hid_dim \n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.pf_dim = pf_dim \n",
        "        self.encoder_layer = encoder_layer\n",
        "        self.self_attention = self_attention\n",
        "        self.positionwise_feedforward = positionwise_feedforward\n",
        "        self.positional_encoding = positional_encoding\n",
        "        self.device = device\n",
        "\n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(1000, hid_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList([encoder_layer(hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device) for _ in range(n_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "    \n",
        "    def forward(self, src, src_mask):\n",
        "\n",
        "        src = self.dropout( (self.tok_embedding(src)*self.scale) )\n",
        "        src = self.positional_encoding(src)\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "        return src\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VodzPexN1IC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward,\n",
        "                 dropout, device):\n",
        "        super().__init__()\n",
        "        self.sa = self_attention(hid_dim, n_heads, dropout, device)\n",
        "        self.ea = self_attention(hid_dim, n_heads, dropout, device)\n",
        "        self.pf = positionwise_feedforward(hid_dim, pf_dim, dropout)\n",
        "        self.ln = nn.LayerNorm(hid_dim)\n",
        "        self.do = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, trg, src, trg_mask, src_mask):\n",
        "        trg = self.ln( trg + self.do( self.sa(trg, trg, trg, trg_mask) ))\n",
        "        trg = self.ln( trg + self.do( self.ea(trg, src, src, src_mask) ))\n",
        "        trg = self.ln(trg + self.do(self.pf(trg)))\n",
        "        return trg"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhE30VzSPuM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, pf_dim,\n",
        "                 decoder_layer, self_attention, positionwise_feedforward, positional_encoding, dropout, devic):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(1000, hid_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [decoder_layer(hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device) for _ in range(n_layers)]\n",
        "        )\n",
        "        self.fc = nn.Linear(hid_dim, output_dim)\n",
        "        self.do = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "      \n",
        "    def forward(self, trg, src, trg_mask, src_mask):\n",
        "\n",
        "        trg = self.do((  self.tok_embedding(trg) * self.scale ))\n",
        "        trg = self.positional_encoding(trg)\n",
        "        for layer in self.layers:\n",
        "            trg = layer(trg, src, trg_mask, src_mask)\n",
        "        trg = self.fc(trg)\n",
        "        return trg"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKkLlIkiQ_W5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder, decoder, pad_idx, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.pad_idx = pad_idx\n",
        "        self.device = device \n",
        "    \n",
        "    def make_masks(self, src, trg):\n",
        "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        trg_pad_mask = (trg != self.pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), dtype=torch.uint8, device=self.device))\n",
        "\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "\n",
        "        return src_mask, trg_mask\n",
        "    \n",
        "    def forward(self, src, trg):\n",
        "        src_mask, trg_mask = self.make_masks(src, trg)\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        out = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        return out "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmcbh6wjRvse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = len(SRC.vocab)\n",
        "output_dim = len(TRG.vocab)\n",
        "hid_dim = 512\n",
        "n_layers = 6\n",
        "n_heads = 8\n",
        "pf_dim = 2048\n",
        "dropout = 0.1\n",
        "pad_idx = SRC.vocab.stoi['<pad>']\n",
        "\n",
        "PE = PositionEncoding(hid_dim, dropout, device)\n",
        "enc = Encoder(input_dim, hid_dim, n_layers, n_heads, pf_dim, EncoderLayer, SelfAttention, PositionwiseFeedforward, PE, dropout, device)\n",
        "dec = Decoder(output_dim, hid_dim, n_layers, n_heads, pf_dim, DecoderLayer, SelfAttention, PositionwiseFeedforward, PE, dropout, device)\n",
        "model = Transformer(enc, dec, pad_idx, device).to(device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffEP6aGHSZ05",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4bf4f98c-ef8a-4794-d709-d07c055cbdb2"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"The model has {count_parameters(model) } trainable parameters\")\n",
        "\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 55206149 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ1oW-OmSdrD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup ** (-1.5)))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGvGfNISSiCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = NoamOpt(hid_dim, 1, 2000, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCv3IXuxSkZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fTom6BkSrBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg[:,:-1])\n",
        "                \n",
        "        #output = [batch size, trg sent len - 1, output dim]\n",
        "        #trg = [batch size, trg sent len]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output.shape[-1])\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "                \n",
        "        #output = [batch size * trg sent len - 1, output dim]\n",
        "        #trg = [batch size * trg sent len - 1]\n",
        "            \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRMD8JtRSwLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg sent len - 1, output dim]\n",
        "            #trg = [batch size, trg sent len]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output.shape[-1])\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            #output = [batch size * trg sent len - 1, output dim]\n",
        "            #trg = [batch size * trg sent len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB5Jq5ZUS1-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cltt8f7XS28n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "a7c0fa5b-2be9-48ba-82e1-6ba06f2bbea2"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "SAVE_DIR = '.'\n",
        "MODEL_SAVE_PATH = os.path.join(SAVE_DIR, 'transformer-seq2seq.pt')\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "if not os.path.isdir(f'{SAVE_DIR}'):\n",
        "    os.makedirs(f'{SAVE_DIR}')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "    \n",
        "    print(f'| Epoch: {epoch+1:03} | Time: {epoch_mins}m {epoch_secs}s| Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Val. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):7.3f} |')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-aff012c835b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-5c0d2a1cf305>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#output = [batch size, trg sent len - 1, output dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-5d26ba6dff95>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0menc_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-5d26ba6dff95>\u001b[0m in \u001b[0;36mmake_masks\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtrg_sub_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtril\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtrg_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrg_pad_mask\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mtrg_sub_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: result type Byte can't be cast to the desired output type Bool"
          ]
        }
      ]
    }
  ]
}